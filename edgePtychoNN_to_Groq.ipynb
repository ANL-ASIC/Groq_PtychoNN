{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253539:INFO:[001.57GB/566.58GB]:<frozen importlib._bootstrap>:688 GroqAPI log level: INFO\n",
      "253539:INFO:[001.57GB/566.58GB]:<frozen importlib._bootstrap>:688 random seed: 1118292051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Authorization required, but no authorization protocol specified\n",
      "Authorization required, but no authorization protocol specified\n",
      "Authorization required, but no authorization protocol specified\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import groq.api as g\n",
    "import groq.runner.tsp as tsp\n",
    "from groqflow import groqit\n",
    "import onnxruntime as ort\n",
    "from groq.runtime import driver as runtime\n",
    "import groq.runtime\n",
    "import time, timeit\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys, re\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Quantizer(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, n_bits):\n",
    "        \"\"\"Quantizes the input x to n_bits precision.\"\"\"\n",
    "        qmin, qmax = 0, 2 ** n_bits - 1\n",
    "        scale = (x.max() - x.min()) / (qmax - qmin)\n",
    "        x_q = torch.round(x / scale).clamp(qmin, qmax) * scale\n",
    "        return x_q\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"Straight-through estimator for gradient computation.\"\"\"\n",
    "        return grad_output, None\n",
    "\n",
    "class ReconSmallPhaseModel(nn.Module):\n",
    "    def __init__(self, nconv: int = 16, n_bits: int = 8):\n",
    "        super(ReconSmallPhaseModel, self).__init__()\n",
    "        self.nconv = nconv\n",
    "        self.n_bits = n_bits  # Number of bits for quantization\n",
    "        self.encoder = nn.Sequential( # Appears sequential has similar functionality as TF avoiding need for separate model definition and activ\n",
    "            *self.down_block(1, self.nconv),\n",
    "            *self.down_block(self.nconv, self.nconv * 2),\n",
    "            *self.down_block(self.nconv * 2, self.nconv * 4),\n",
    "            *self.down_block(self.nconv * 4, self.nconv * 8), \n",
    "            *self.down_block(self.nconv * 8, self.nconv * 16), \n",
    "            *self.down_block(self.nconv * 16, self.nconv * 32),\n",
    "            #*self.down_block(self.nconv * 32, self.nconv * 32)\n",
    "        )\n",
    "        \n",
    "        # amplitude model\n",
    "        #self.decoder1 = nn.Sequential(\n",
    "            #*self.up_block(self.nconv * 32, self.nconv * 32),\n",
    "         #   *self.up_block(self.nconv * 32, self.nconv * 16),\n",
    "          #  *self.up_block(self.nconv * 16, self.nconv * 8),\n",
    "           # *self.up_block(self.nconv * 8, self.nconv * 8),\n",
    "            #*self.up_block(self.nconv * 8, self.nconv * 4),\n",
    "            #*self.up_block(self.nconv * 4, self.nconv * 2),\n",
    "            #*self.up_block(self.nconv * 2, self.nconv * 1),\n",
    "            #nn.Conv2d(self.nconv * 1, 1, 3, stride=1, padding=(1,1)),\n",
    "        #)\n",
    "        \n",
    "        # phase model\n",
    "        self.decoder2 = nn.Sequential(\n",
    "            #*self.up_block(self.nconv * 32, self.nconv * 32),\n",
    "            *self.up_block(self.nconv * 32, self.nconv * 16),\n",
    "            *self.up_block(self.nconv * 16, self.nconv * 8),\n",
    "            *self.up_block(self.nconv * 8, self.nconv * 8),\n",
    "            *self.up_block(self.nconv * 8, self.nconv * 4),\n",
    "            *self.up_block(self.nconv * 4, self.nconv * 2),\n",
    "            *self.up_block(self.nconv * 2, self.nconv * 1),\n",
    "            nn.Conv2d(self.nconv * 1, 1, 3, stride=1, padding=(1,1)),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def down_block(self, filters_in, filters_out):\n",
    "        block = [\n",
    "            nn.Conv2d(in_channels=filters_in, out_channels=filters_out, kernel_size=3, stride=1, padding=(1,1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(filters_out, filters_out, 3, stride=1, padding=(1,1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2,2))\n",
    "        ]\n",
    "        return block\n",
    "    \n",
    "    \n",
    "    def up_block(self, filters_in, filters_out):\n",
    "        block = [\n",
    "            nn.Conv2d(filters_in, filters_out, 3, stride=1, padding=(1,1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(filters_out, filters_out, 3, stride=1, padding=(1,1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        ]\n",
    "        return block\n",
    "        \n",
    "    \n",
    "    def forward(self,x):\n",
    "        with torch.cuda.amp.autocast():\n",
    "            x = Quantizer.apply(x,self.n_bits)    # Apply quantization to input\n",
    "            #print(\"Data type after quantization of inputs:\", x.dtype)   # Print the data type after quantization\n",
    "            \n",
    "            x1 = self.encoder(x)\n",
    "            x1 = Quantizer.apply(x1, self.n_bits) # Quantize feature maps\n",
    "            #print(\"Size after encoder:\", x1.size())  # Print size after encoder\n",
    "            #print(\"Type after encoder:\", x1.dtype)   # Print the data type after quantization\n",
    "            \n",
    "            #amp = self.decoder1(x1)\n",
    "            ph = self.decoder2(x1)\n",
    "            ph = Quantizer.apply(ph, self.n_bits)  #Quantize output\n",
    "\n",
    "            #Restore -pi to pi range\n",
    "            ph = ph*np.pi #Using tanh activation (-1 to 1) for phase so multiply by pi\n",
    "            \n",
    "        return ph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PtychographyDataset(Dataset):\n",
    "    def __init__(self, npz_files):\n",
    "\n",
    "        # Initialize empty lists for inputs and labels from all files\n",
    "        all_inputs = []\n",
    "        all_labels = []\n",
    "\n",
    "        # Loop through all 10 files\n",
    "        for npz_file in npz_files:\n",
    "            data = np.load(npz_file)\n",
    "            inputs = torch.tensor(data[\"reciprocal\"], dtype=torch.float32)\n",
    "            labels = np.angle(data[\"real\"])\n",
    "            labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "            # Ensure input and label shapes are correct \n",
    "            if len(inputs.shape) == 3:  \n",
    "                inputs = inputs.unsqueeze(1)  # Add channel dim (C=1)\n",
    "                labels = labels.unsqueeze(1)  # Add channel dim (C=1)\n",
    "\n",
    "            # Append the data from file to lists\n",
    "            all_inputs.append(inputs)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "        # Concatenate all data \n",
    "        self.inputs = torch.cat(all_inputs, dim = 0)\n",
    "        self.labels = torch.cat(all_labels, dim = 0)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.labels[idx]\n",
    "    \n",
    "npz_file = [\"/home/sfowler/Downloads/NewScans/scan780.npz\"]\n",
    "checkpoint_path = \"checkpoint_q.pth\"\n",
    "\n",
    "dataset = PtychographyDataset(npz_file)\n",
    "\n",
    "# Create a DataLoader for the test dataset\n",
    "test_dataloader = DataLoader(dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export To Onnx and IOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input image shape: torch.Size([1, 1, 128, 128])\n",
      "current_directory: /home/sfowler/Groq_PtychoNN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sfowler/miniconda3/envs/newGroq/lib/python3.10/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Get a single input image tensor from the dataset\n",
    "inputs, _ = next(iter(test_dataloader))  # Get the first batch\n",
    "input_im_tensor = inputs[0].unsqueeze(0)  # Extract the first image from the batch\n",
    "\n",
    "# Print the shape to verify it's correct\n",
    "print(f\"Input image shape: {input_im_tensor.shape}\")\n",
    "\n",
    "input_size = 128\n",
    "model = ReconSmallPhaseModel()\n",
    "onnx_model = \"/home/sfowler/Groq_PtychoNN/Onnx/quantized_PtychoNN.onnx\"\n",
    "iop_file_path = \"/home/sfowler/Groq_PtychoNN/IOP/quantized_PtychoNN.iop\"\n",
    "\n",
    "# Get current directory and print to the screen\n",
    "current_directory = os.getcwd()\n",
    "print(f\"current_directory: {current_directory}\")\n",
    "\n",
    "\n",
    "# Export PyTorch Model to ONNX\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    input_im_tensor,\n",
    "    onnx_model,\n",
    "    opset_version=14,\n",
    "    input_names=[\"input_image\"],\n",
    "    output_names=[\"output\"],\n",
    ")\n",
    "\n",
    "\n",
    "onnx_model_path = \"/home/sfowler/Groq_PtychoNN/Onnx/quantized_PtychoNN.onnx\"\n",
    "iop_file_path = \"/home/sfowler/Groq_PtychoNN/IOP/quantized_PtychoNN.iop\"\n",
    "\n",
    "# Use GroqFlow to compile the model\n",
    "# try:\n",
    "#     # groqit automatically compiles the model and returns the package path\n",
    "#     package_path = groqit(onnx_model_path, inputs={\"input_image\": input_im_tensor}, cache_dir=os.path.join(current_directory, \"../.cache\"), groqview = True, rebuild=\"always\", build_name = \"edgePtychoNN\")\n",
    "\n",
    "#     # The .iop file is located in the generated package directory\n",
    "#     print(f\"Compilation successful. IOP file saved to: {iop_file_path}\")\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"Error during compilation: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newGroq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
